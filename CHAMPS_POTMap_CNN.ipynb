{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "#### CNN for CHAMPS Data Set ####\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import functools as ft\n",
    "import time as t\n",
    "import pickle\n",
    "# ML Packages\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "# Neural Net Stuff\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/\n",
    "!mkdir ./logs/\n",
    "\n",
    "# Constrain GPU Memory Usage\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Import and Initialize Parallel Pandas Stuff\n",
    "import dask.dataframe as ddk\n",
    "\n",
    "# Debug Stuff\n",
    "# from IPython.core.debugger import set_trace\n",
    "\n",
    "# Global Variable Declarations\n",
    "f_path = \"~/Desktop/Kaggle_Data/CHAMPS/\" # local location of Kaggle Data\n",
    "numpy_fpath = \"/home/aj/Desktop/Kaggle_Data/CHAMPS/NUMPY_ARRAYS/\" # local numpy array save spot\n",
    "step_size = 0.5 # step size for building position array and model inputs\n",
    "steps = np.arange(0, 30, step_size)\n",
    "column_dict = {'1': 'rank_1', '2': 'rank_2', '3': 'rank_3', 'HH': 'pair_HH', 'HC': 'pair_HC', 'HN': 'pair_HN'} # For Building Atom Type and Coupling Type Features\n",
    "\n",
    "#### Load Position Matrix if On File; Otherwise Build It ####\n",
    "try:\n",
    "    position_matrix = np.load(numpy_fpath + \"position_matrix_\" + str(step_size) + \"step_size.npy\")\n",
    "except IOError:\n",
    "    position_matrix = np.zeros((len(steps), len(steps), len(steps), 3), dtype=np.float32)\n",
    "    for ijk in np.ndindex(position_matrix.shape[:3]):\n",
    "        position_matrix[ijk] = np.asarray(ijk, dtype=np.float32)*step_size\n",
    "    np.save(numpy_fpath + \"position_matrix_\" + str(step_size) + \"step_size.npy\", position_matrix)\n",
    "##############################################################\n",
    "\n",
    "# Function for Potential Calculations\n",
    "# potential_function = np.vectorize(lambda x: 0.8909*(np.power((0.8909/x),12)-np.power((0.8909/x), 6)) if x > 0.817794 else np.float32(1.05)) # The 1.05 is to represent a non-physical region\n",
    "# Global Factors\n",
    "pot_function_factor = np.float32(0.8909)\n",
    "pot_factor_12 = np.power(pot_function_factor, 12)\n",
    "pot_factor_6 = np.power(pot_function_factor, 6)\n",
    "\n",
    "# Callback Functions for Tensorboard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Global Names for Saves & Callbacks\n",
    "model_name = 'CHAMPS_StandardModel_ss_' + str(step_size)\n",
    "model_epochsteps = '100e50s'\n",
    "model_batchsize = 'batch5'\n",
    "model_date = '07162020'\n",
    "\n",
    "# For Multiprocessing Error Bypass (Why is it Even Happening??)\n",
    "os.sys.modules['__main__'].__spec__ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aj/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# Load in Data\n",
    "training_df = pd.read_csv(f_path + \"train.csv\", index_col='id')\n",
    "structures_df = pd.read_csv(f_path + \"structures.csv\") # Structure Data Treated As Global; Includes All Molecules, even Test Set\n",
    "# When you're ready to test\n",
    "# testing_df = pd.read_csv(f_path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################################\n",
      "      molecule_name  scalar_coupling_constant  rank_1  rank_2  rank_3  \\\n",
      "0  dsgdb9nsd_000001                   84.8076       1       0       0   \n",
      "1  dsgdb9nsd_000001                  -11.2570       0       1       0   \n",
      "2  dsgdb9nsd_000001                  -11.2548       0       1       0   \n",
      "3  dsgdb9nsd_000001                  -11.2543       0       1       0   \n",
      "4  dsgdb9nsd_000001                   84.8074       1       0       0   \n",
      "\n",
      "   pair_HH  pair_HC  pair_HN  \n",
      "0        0        1        0  \n",
      "1        1        0        0  \n",
      "2        1        0        0  \n",
      "3        1        0        0  \n",
      "4        0        1        0  \n",
      "#####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Some of the Datafram\n",
    "\n",
    "# Get List of Unique Molecules\n",
    "unique_molecules = training_df['molecule_name'].unique()\n",
    "\n",
    "# Map the Coupling info Into the DataFrame\n",
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures_df, how = 'left', left_on = ['molecule_name', f'atom_index_{atom_idx}'], right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    # Rename Columns as Needed\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}', 'x': f'x_{atom_idx}', 'y': f'y_{atom_idx}', 'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "def one_hot_encoding_for_rank_pair(series):\n",
    "    type_split = series['type'].split('J')\n",
    "    series[column_dict[type_split[0]]] = 1\n",
    "    series[column_dict[type_split[1]]] = 1\n",
    "    return series\n",
    "\n",
    "def build_champs_features(df):\n",
    "    # Map Training DataFrame to Structure Info\n",
    "    df = map_atom_info(df, 0)\n",
    "    df = map_atom_info(df, 1)\n",
    "\n",
    "    # Add Atomic Distance Feature\n",
    "    vector_atom_0 = df[['x_0','y_0','z_0']].values\n",
    "    vector_atom_1 = df[['x_1','y_1','z_1']].values\n",
    "    df['atom_dist'] = np.linalg.norm(vector_atom_0 - vector_atom_1, axis=1)\n",
    "\n",
    "    # Move Origin For Image Building Purposes\n",
    "    df[['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1']] = df[['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1']]+15\n",
    "\n",
    "    # One-Hot Encode Unique Types\n",
    "    df = df.assign(**pd.DataFrame(0, columns=['rank_1', 'rank_2', 'rank_3', 'pair_HH', 'pair_HC', 'pair_HN'], index=training_df.index))\n",
    "    # Parallalize the Apply on Dataframe\n",
    "    dask_df = ddk.from_pandas(df, mp.cpu_count())\n",
    "    df = dask_df.apply(one_hot_encoding_for_rank_pair, axis=1, meta=dask_df).compute(scheduler='processes')\n",
    "           \n",
    "    return df\n",
    "\n",
    "# Build Features On Training Set\n",
    "training_df = build_champs_features(training_df)\n",
    "\n",
    "print(\"#####################################################################################################\")\n",
    "print(training_df[['molecule_name', 'scalar_coupling_constant', 'rank_1', 'rank_2', 'rank_3', 'pair_HH', 'pair_HC', 'pair_HN']].head())\n",
    "feat_toKeep = ['atom_dist', 'rank_1', 'rank_2', 'rank_3', 'pair_HH', 'pair_HC', 'pair_HN']\n",
    "print(\"#####################################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Build Batch of Images\n",
    "def build_potmap(rest, series):\n",
    "    # Get Pair Vectors from Series Passed In\n",
    "    pair_v = [series[['x_0', 'y_0', 'z_0']].values.tolist(), series[['x_1', 'y_1', 'z_1']].values.tolist()]\n",
    "    # Start Building Images\n",
    "    others_v = [x for x in rest if x not in pair_v]\n",
    "\n",
    "    # Track Time\n",
    "    # start_time = t.perf_counter()\n",
    "    # Build Potential Map (Dual Channel)\n",
    "    others_potmap = np.zeros((len(steps), len(steps), len(steps)), dtype=np.float32)\n",
    "    for o_vec in others_v:\n",
    "        # For Modified Lennard Jones Potential\n",
    "        norm_tensor = tf.norm(tf.subtract(position_matrix, np.asarray(o_vec)), axis=3)\n",
    "        pot_map_calc = tf.multiply(tf.subtract(tf.multiply(tf.math.reciprocal_no_nan(tf.pow(norm_tensor, np.float32(12))), pot_factor_12), tf.multiply(tf.math.reciprocal_no_nan(tf.pow(norm_tensor, np.float32(6))), pot_factor_6)), pot_function_factor)\n",
    "        pot_map_decide = tf.where(norm_tensor < np.float32(0.817794), np.float32(1.05), pot_map_calc)\n",
    "        others_potmap = tf.add(others_potmap, pot_map_decide)\n",
    "        \n",
    "        # others_potmap = np.add(others_potmap, potential_function(np.linalg.norm(np.subtract(position_matrix, np.asarray(o_vec)), axis=3)))\n",
    "    \n",
    "    pair_potmap = np.zeros((len(steps), len(steps), len(steps)), dtype=np.float32)\n",
    "    for p_vec in pair_v:\n",
    "        # For Modified Lennard Jones Potential\n",
    "        norm_tensor = tf.norm(tf.subtract(position_matrix, np.asarray(p_vec)), axis=3)\n",
    "        pot_map_calc = tf.multiply(tf.subtract(tf.multiply(tf.math.reciprocal_no_nan(tf.pow(norm_tensor, np.float32(12))), pot_factor_12), tf.multiply(tf.math.reciprocal_no_nan(tf.pow(norm_tensor, np.float32(6))), pot_factor_6)), pot_function_factor)\n",
    "        pot_map_decide = tf.where(norm_tensor < np.float32(0.817794), np.float32(1.05), pot_map_calc)\n",
    "        pair_potmap = tf.add(pair_potmap, pot_map_decide)\n",
    "        \n",
    "        # pair_potmap = np.add(pair_potmap, potential_function(np.linalg.norm(np.subtract(position_matrix, np.asarray(p_vec)), axis=3)))\n",
    "    \n",
    "    pot_total = np.stack((pair_potmap, others_potmap), axis=-1)\n",
    "    \n",
    "    # Save Dual Channel \"Picture\" to Disk\n",
    "    # tmp_pic_name = numpy_fpath + \"potential_map_\" + unique_molecules[unique_mol_index] + \"_\" + \"PairIndex_\" + str(atm_index1) + \"_\" + str(atm_index2) + \"__\" + str(step_size) + \"_step.npy\"\n",
    "    # np.save(tmp_pic_name, pot_total)\n",
    "\n",
    "    # Track Time\n",
    "    # end_time = t.perf_counter()\n",
    "    # print(f'Processing Time for Potential Map of Pair {pair_v}: {np.round(end_time-start_time, 2)} Seconds')\n",
    "    series['pot_map'] = pot_total\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Build Batch for CNN Training\n",
    "def build_batch(df, from_file=False):\n",
    "    if from_file == False:\n",
    "        while True:\n",
    "            with tf.device('/cpu:0'): # Do Data Building and Large Memory Storage on CPU Side\n",
    "                the_data = create_data(df)\n",
    "                for _ in range(3):\n",
    "                    the_data = the_data.concatenate(create_data(df))\n",
    "\n",
    "                the_data = the_data.shuffle(tf.data.experimental.cardinality(the_data))\n",
    "            # Here is Iterator for Data Generation During Training\n",
    "            for i in the_data.batch(5):\n",
    "                yield i\n",
    "    else:\n",
    "        file_iterator = 0\n",
    "        while True:\n",
    "            fromFile_df = pd.read_pickle('~/Desktop/CHAMPS_DataOnDisk/CHAMPS_PotMap_Included_' + str(file_iterator) +'_.pkl')\n",
    "            input_dataset = tf.data.Dataset.from_tensor_slices({'feat_input':fromFile_df[feat_toKeep].astype(np.float32).values, 'pic_input': tf.stack(fromFile_df.pop('pot_map').tolist())})\n",
    "            target_dataset = tf.data.Dataset.from_tensor_slices(fromFile_df['scalar_coupling_constant'].astype(np.float32).values)\n",
    "            dataset_fromFile = tf.data.Dataset.zip((input_dataset, target_dataset))\n",
    "            dataset_fromFile = dataset_fromFile.shuffle(tf.data.experimental.cardinality(dataset_fromFile))\n",
    "            file_iterator = file_iterator + 1\n",
    "            for i in dataset_fromFile.batch(5):\n",
    "                yield i\n",
    "\n",
    "def create_data(df, which_set='train'):\n",
    "    # Check if Train, Test, or Validation\n",
    "    if which_set == 'test':\n",
    "        i = np.random.randint(69001, 85012)\n",
    "    elif which_set == 'val':\n",
    "        i = np.random.randint(68000, 69000)\n",
    "    else:\n",
    "        i = np.random.randint(0, 68000)\n",
    "    \n",
    "    # Choose Subset of Data\n",
    "    training_subset = df[df['molecule_name'] == unique_molecules[i]].copy()\n",
    "    # Get All Vector Positions of Atoms\n",
    "    rest_of_vectors = np.unique(np.concatenate((training_subset[['x_0', 'y_0', 'z_0']].values, training_subset[['x_1', 'y_1', 'z_1']].values), axis=0), axis=0)\n",
    "    # Define a Partial Where rest_of_vectors Doesn't Change\n",
    "    build_potmap_partial = ft.partial(build_potmap, rest_of_vectors.tolist())\n",
    "    # print(\"######################################################################\")\n",
    "    # print(f'Batch has {len(rest_of_vectors)} Atoms in Molecule.')\n",
    "    # print(\"######################################################################\")\n",
    "\n",
    "    # Call Total Vectorized Function to Build Batch Pot Images\n",
    "    training_subset['pot_map'] = 0.0\n",
    "    \n",
    "    # start_time = t.perf_counter()\n",
    "    #######################################################################################\n",
    "    # if training_subset.shape[0] > mp.cpu_count():\n",
    "    #     dask_df = ddk.from_pandas(training_subset, mp.cpu_count())\n",
    "    # else:\n",
    "    #     dask_df = ddk.from_pandas(training_subset, training_subset.shape[0])\n",
    "    \n",
    "    # training_subset = dask_df.apply(build_potmap_partial, axis=1, meta=dask_df).compute(scheduler='processes')\n",
    "    training_subset = training_subset.apply(build_potmap_partial, axis=1)\n",
    "    ########################################################################################\n",
    "    # end_time = t.perf_counter()\n",
    "    # print(\"Creating Data took {} Seconds\".format(np.round(end_time-start_time, 2)))\n",
    "    \n",
    "    # print(\"######################################################################\")\n",
    "    # print(\"Number of Pairs Calculated: {}\".format(len(training_subset['pot_map'])))\n",
    "    # print(\"######################################################################\")\n",
    "    \n",
    "    if which_set != 'save':\n",
    "        input_dataset = tf.data.Dataset.from_tensor_slices({'feat_input':training_subset[feat_toKeep].astype(np.float32).values, 'pic_input': tf.stack(training_subset.pop('pot_map').tolist())})\n",
    "        target_dataset = tf.data.Dataset.from_tensor_slices(training_subset['scalar_coupling_constant'].astype(np.float32).values)\n",
    "        zipped_dataset = tf.data.Dataset.zip((input_dataset, target_dataset))\n",
    "    \n",
    "    # Return Appropriate Result for which_set\n",
    "    if (which_set == 'test') or (which_set == 'val'):\n",
    "        return zipped_dataset.batch(10)\n",
    "    elif (which_set == 'save'):\n",
    "        return training_subset\n",
    "    else:\n",
    "        return zipped_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Build Data to Put on Disk to Free up GPU Memory During Training.\n",
    "def build_data_for_disk(df, num_sets=5):\n",
    "    print(\"Building Data to Put on HDD...\")\n",
    "    for i in range(num_sets):# To Build Data on Disk; Need 37500 items for Full Training\n",
    "        building_data = create_data(df, 'save')\n",
    "        while building_data.shape[0] < 5000:\n",
    "            # start_time = t.perf_counter()\n",
    "            building_data = pd.concat([building_data, create_data(df, 'save')])\n",
    "            # print(\"Iteration Passed...\")\n",
    "            # print(\"Size of Building_Data: {}\".format(building_data.shape[0]))\n",
    "            # end_time = t.perf_counter()\n",
    "            # print(\"It took {} seconds.\".format(np.round(end_time-start_time, 2)))\n",
    "    \n",
    "        building_data.to_pickle('~/Desktop/CHAMPS_DataOnDisk/CHAMPS_PotMap_Included_' + str(i) + '_SS' + step_size +'.pkl') # Five is Interference for Start of File Name after Some Files already Created\n",
    "        del building_data\n",
    "    print(\"Done Building Data to Put on HDD.\")\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2181.32 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "# Define CNN Keras Model for CHAMPS Data\n",
    "def build_cnn_model(m_type='cnn_standard'):\n",
    "    # Build ML Model Here;\n",
    "    if m_type == 'cnn_standard':\n",
    "        # Build a simple model so input types can be compared for performance\n",
    "        # Define CNN Keras Model for CHAMPS Data\n",
    "        input_desc = tf.keras.layers.Input(shape=(7,), name=\"feat_input\")\n",
    "        # hidden_desc1 = tf.keras.layers.Dense(50, activation=\"relu\")(input_desc)\n",
    "        # dropout_1 = tf.keras.layers.Dropout(0.25)(hidden_desc1)\n",
    "        # hidden_desc2 = tf.keras.layers.Dense(25, activation=\"relu\")(dropout_1)\n",
    "        # Input for Potential Map Picture\n",
    "        input_pic = tf.keras.layers.Input(shape=(len(steps), len(steps), len(steps), 2), name=\"pic_input\")\n",
    "        conv_pic1 = tf.keras.layers.Conv3D(16, 3, padding=\"same\", activation=\"relu\", data_format=\"channels_last\")(input_pic)\n",
    "        conv_pic2 = tf.keras.layers.Conv3D(32, 3, padding=\"same\", activation=\"relu\")(conv_pic1)\n",
    "        # conv_pool1 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2), padding=\"same\", data_format=\"channels_last\")(conv_pic2)\n",
    "        conv_pic3 = tf.keras.layers.Conv3D(64, 3, padding=\"same\", activation=\"relu\")(conv_pic2) # (conv_pool1)\n",
    "        conv_pic4 = tf.keras.layers.Conv3D(128, 3, padding=\"same\", activation=\"relu\")(conv_pic3)\n",
    "        # conv_pool2 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2), padding=\"same\", data_format=\"channels_last\")(conv_pic4)\n",
    "        flatten_conv = tf.keras.layers.Flatten()(conv_pic4) # (conv_pool2)\n",
    "        # Concatenate the Layers and Move to Output\n",
    "        # conv_toDNN1_flatten = tf.keras.layers.Flatten()(conv_pool1)\n",
    "        # conv_toDNN1 = tf.keras.layers.Dense(50, activation=\"relu\")(conv_toDNN1_flatten)\n",
    "        # concat = tf.keras.layers.Concatenate(axis=1)([hidden_desc2, flatten_conv, conv_toDNN1])\n",
    "        # preout3 = tf.keras.layers.Dense(100, activation=\"relu\")(concat)\n",
    "        # preout_dropout1 = tf.keras.layers.Dropout(0.25)(preout3)\n",
    "        # preout2 = tf.keras.layers.Dense(50, activation=\"relu\")(preout_dropout1)\n",
    "        # preout_dropout2 = tf.keras.layers.Dropout(0.25)(preout2)\n",
    "        # preout1 = tf.keras.layers.Dense(25, activation=\"relu\")(preout_dropout2)\n",
    "        output = tf.keras.layers.Dense(1, name=\"output\")(flatten_conv) # (preout1)\n",
    "        c_model = tf.keras.Model(inputs=[input_desc, input_pic], outputs=output)\n",
    "        c_model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "    else:\n",
    "        # Input for Coupling Type, Etc.\n",
    "        input_desc = tf.keras.layers.Input(shape=(7,), name=\"feat_input\")\n",
    "        hidden_desc1 = tf.keras.layers.Dense(50, activation=\"relu\")(input_desc)\n",
    "        dropout_1 = tf.keras.layers.Dropout(0.25)(hidden_desc1)\n",
    "        hidden_desc2 = tf.keras.layers.Dense(25, activation=\"relu\")(dropout_1)\n",
    "        # Input for Potential Map Picture\n",
    "        input_pic = tf.keras.layers.Input(shape=(len(steps), len(steps), len(steps), 2), name=\"pic_input\")\n",
    "        conv_pic1 = tf.keras.layers.Conv3D(16, 3, padding=\"same\", activation=\"relu\", data_format=\"channels_last\")(input_pic)\n",
    "        conv_pic2 = tf.keras.layers.Conv3D(32, 3, padding=\"same\", activation=\"relu\")(conv_pic1)\n",
    "        conv_pool1 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2), padding=\"same\", data_format=\"channels_last\")(conv_pic2)\n",
    "        conv_pic3 = tf.keras.layers.Conv3D(64, 3, padding=\"same\", activation=\"relu\")(conv_pool1)\n",
    "        conv_pic4 = tf.keras.layers.Conv3D(128, 3, padding=\"same\", activation=\"relu\")(conv_pic3)\n",
    "        conv_pool2 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2), padding=\"same\", data_format=\"channels_last\")(conv_pic4)\n",
    "        flatten_conv = tf.keras.layers.Flatten()(conv_pool2)\n",
    "        # Concatenate the Layers and Move to Output\n",
    "        conv_toDNN1_flatten = tf.keras.layers.Flatten()(conv_pool1)\n",
    "        conv_toDNN1 = tf.keras.layers.Dense(50, activation=\"relu\")(conv_toDNN1_flatten)\n",
    "        concat = tf.keras.layers.Concatenate(axis=1)([hidden_desc2, flatten_conv, conv_toDNN1])\n",
    "        preout3 = tf.keras.layers.Dense(100, activation=\"relu\")(concat)\n",
    "        preout_dropout1 = tf.keras.layers.Dropout(0.25)(preout3)\n",
    "        preout2 = tf.keras.layers.Dense(50, activation=\"relu\")(preout_dropout1)\n",
    "        preout_dropout2 = tf.keras.layers.Dropout(0.25)(preout2)\n",
    "        preout1 = tf.keras.layers.Dense(25, activation=\"relu\")(preout_dropout2)\n",
    "        output = tf.keras.layers.Dense(1, name=\"output\")(preout1)\n",
    "        c_model = tf.keras.Model(inputs=[input_desc, input_pic], outputs=output)\n",
    "        c_model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "    \n",
    "    return c_model\n",
    "\n",
    "# champs_model = build_cnn_model('Something_Else') # This is for Original Model; Testing a Model itself and Not Holding it Constant for Investigation of How Energy Resolution or Such Affects Results\n",
    "champs_model = build_cnn_model() # Testing Energy Resolution & Such (Only Includes Energy Map in Input & Leaves out Coupling Type, etc.)\n",
    "%memit champs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cbe489a07c4ebdd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cbe489a07c4ebdd3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Validation Data...\n",
      "Training Model Input... \n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 50 steps, validate for 6 steps\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 100s 2s/step - loss: 20.0490 - val_loss: 33.1207\n",
      "Epoch 2/100\n",
      "32/50 [==================>...........] - ETA: 24s - loss: 17.8747"
     ]
    }
   ],
   "source": [
    "# Decide if Will Build Data to Put on HDD\n",
    "# build_data_for_disk(training_df, 10) # This is Functional Call to Build Data on Disk\n",
    "# Callback Function for Model Save\n",
    "checkpoint_saves = tf.keras.callbacks.ModelCheckpoint(model_name + \"_\" + model_epochsteps + \"_\" + model_batchsize + \"_\" + model_date + \"_model.h5\", save_best_only=True)\n",
    "# Get Validation Data for Training Model\n",
    "print(\"Get Validation Data...\")\n",
    "val_data = create_data(training_df, 'val')\n",
    "# Train Model\n",
    "print(\"Training Model Input... \")\n",
    "# With or Without Callback Save Points\n",
    "history = champs_model.fit(build_batch(training_df, False), epochs=100, steps_per_epoch=50, validation_data=val_data, callbacks=[checkpoint_saves, tensorboard_callback]) #, use_multiprocessing=True, max_queue=50, workers=32)\n",
    "\n",
    "# Save Model History for Graphing Data Later\n",
    "with open(model_name + \"_\" + model_epochsteps + \"_\" + model_batchsize + \"_\" + model_date + \"_model_history\", 'wb') as model_history:\n",
    "    pickle.dump(history.history, model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Things\n",
    "plt.plot(history.history['loss'][:], label='Mean Absolute Error (Training Data)')\n",
    "plt.plot(history.history['val_loss'][:], label='Mean Absolute Error (Val Data)')\n",
    "plt.title('CHAMPS Conv3D Model Performance')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.xlabel('No. Epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Some Test Cases to See if Model is Moving in Right Direction\n",
    "results_df = pd.DataFrame(data=None, columns=feat_toKeep + ['coupling_const', 'val_pred', 'abs_error'], dtype=np.float32)\n",
    "for bottle in range(500):\n",
    "    predict_batch = create_data(training_df, 'test')\n",
    "    for i, j in predict_batch.take(500):\n",
    "        val_pred = champs_model.predict(i)\n",
    "\n",
    "        j = np.reshape(j, (j.numpy().shape[0],1))\n",
    "        result_matrix = np.concatenate([i['feat_input'].numpy(), j, val_pred, np.abs(val_pred-j)], axis=-1)\n",
    "        results_df = results_df.append(pd.DataFrame(data=result_matrix, columns=feat_toKeep + ['coupling_const', 'val_pred', 'abs_error'], dtype=np.float32))\n",
    "\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_toKeep = ['atom_dist', 'rank_1', 'rank_2', 'rank_3', 'pair_HH', 'pair_HC', 'pair_HN']\n",
    "axes_df = list(range(8))\n",
    "ax = list(range(16))\n",
    "fig = plt.figure(figsize=(15, 25))\n",
    "fig.suptitle('Error Distribution of Prediction vs. Target', fontsize=14, y=0.91)\n",
    "grid = gridspec.GridSpec(8, 2, figure=fig, hspace=0.5)\n",
    "total_score = 0\n",
    "\n",
    "selection_string = [['rank_1', 'pair_HC'], ['rank_1', 'pair_HN'], ['rank_2', 'pair_HH'], ['rank_2', 'pair_HC'], ['rank_2', 'pair_HN'], ['rank_3', 'pair_HH'], ['rank_3', 'pair_HC'], ['rank_3', 'pair_HN']]\n",
    "rank_dict = {'rank_1': 1, 'rank_2': 2, 'rank_3': 3}\n",
    "type_dict = {'pair_HH':'HH', 'pair_HC':'HC', 'pair_HN':'HN'}\n",
    "for n in range(8):\n",
    "    axes_df[n] = results_df[(results_df[selection_string[n][0]] != 0.0) & (results_df[selection_string[n][1]] != 0.0)]\n",
    "    abs_average_error = axes_df[n]['abs_error'].mean()\n",
    "    ax[2*n] = plt.subplot(grid[2*n])\n",
    "    ax[2*n].set_title(\"{}J{} Average Error: {}\".format(rank_dict[selection_string[n][0]], type_dict[selection_string[n][1]], abs_average_error))\n",
    "    sns.distplot(axes_df[n]['abs_error'], ax=ax[2*n])\n",
    "    \n",
    "    ax[2*n+1] = plt.subplot(grid[2*n+1])\n",
    "    ax[2*n+1].set_title(\"{}J{} Prediction v. Target\".format(rank_dict[selection_string[n][0]], type_dict[selection_string[n][1]]))\n",
    "    sns.scatterplot(x=\"coupling_const\", y=\"val_pred\", data=axes_df[n], ax=ax[2*n+1])\n",
    "    \n",
    "    if np.isnan(abs_average_error):\n",
    "        abs_average_error = 1\n",
    "    total_score = total_score + np.log10(abs_average_error)\n",
    "\n",
    "total_score = total_score/8.0\n",
    "plt.savefig('Error_Distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Model Score as Calculated by Kaggle Competition; Winning Score was -3.23968\n",
    "print(\"The Total Score of the Model this Run was: {}\".format(total_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
